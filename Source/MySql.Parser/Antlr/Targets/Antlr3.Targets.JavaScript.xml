<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Antlr3.Targets.JavaScript</name>
    </assembly>
    <members>
        <member name="M:Antlr3.Targets.JavaScriptTarget.EncodeIntAsCharEscape(System.Int32)">
            Convert an int to a JavaScript Unicode character literal.
            
              The current JavaScript spec (ECMA-262) doesn't provide for octal
              notation in String literals, although some implementations support it.
              This method overrides the parent class so that characters will always
              be encoded as Unicode literals (e.g. \u0011).
        </member>
        <member name="M:Antlr3.Targets.JavaScriptTarget.GetTarget64BitStringFromValue(System.UInt64)">
            Convert long to two 32-bit numbers separted by a comma.
              JavaScript does not support 64-bit numbers, so we need to break
              the number into two 32-bit literals to give to the Bit.  A number like
              0xHHHHHHHHLLLLLLLL is broken into the following string:
              "0xLLLLLLLL, 0xHHHHHHHH"
              Note that the low order bits are first, followed by the high order bits.
              This is to match how the BitSet constructor works, where the bits are
              passed in in 32-bit chunks with low-order bits coming first.
            
              Note: stole the following two methods from the ActionScript target.
        </member>
    </members>
</doc>
